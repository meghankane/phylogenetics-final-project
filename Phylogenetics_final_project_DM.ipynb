{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Dqa8zrdcE-5"
   },
   "source": [
    "#0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OCFiJ-F0HKC4"
   },
   "outputs": [],
   "source": [
    "#I. Importing required packages\n",
    "from re import search, compile\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import copy\n",
    "import os\n",
    "import ete3\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#II. Loading the sequence dataset\n",
    "fasta_file_path = \"./influenza_98.fasta\"\n",
    "\n",
    "#III. Defining the mutation cost matrix for maximum parsimony \n",
    "W = np.array([[0, 1, 1, 1], \n",
    "              [1, 0, 1, 1],\n",
    "              [1, 1, 0, 1],\n",
    "              [1, 1, 1, 0]])\n",
    "\n",
    "#IV. Setting the random seed for reproducibility\n",
    "random.seed(1234)\n",
    "\n",
    "#V. Printing messages to a log file\n",
    "#not used in .ipynb\n",
    "# orig_stdout = sys.stdout\n",
    "# sys.stdout = open('test.log', 'a')\n",
    "# sys.stdout = orig_stdout\n",
    "\n",
    "#VI. Creating a data frame storing stats per iteration\n",
    "df = pd.DataFrame(columns=['k', 'MP score of local', 'MST nodes', \"Global tree vertices\", \"Global tree edges\", \"V size\"])\n",
    "\n",
    "#VII. Setting the threshold for inferring local tree\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSf1jZHIHchI"
   },
   "source": [
    "#1. Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f5Nk2unwHhi8"
   },
   "outputs": [],
   "source": [
    "#I. Importing the Tree Class for phylogeny inference\n",
    "class Vertex:\n",
    "  def __init__(self,name): \n",
    "    self.name = name\n",
    "    self.in_degree = 0\n",
    "    self.out_degree = 0\n",
    "    self.parent = self\n",
    "    self.children = []\n",
    "    self.neighbors = []\n",
    "    self.newick_label = \"\"\n",
    "    self.sequence = \"\"\n",
    "    self.min_cost_subtree = []\n",
    "    self.is_leaf = False\n",
    "    self.is_root = False\n",
    "    self.subtree = []\n",
    "\n",
    "class Tree:\n",
    "  def __init__(self,name):\n",
    "    self.name = name\n",
    "    self.vertex_map = {}\n",
    "    self.pre_order_list = []\n",
    "    self.post_order_list = []\n",
    "    self.edge_list_map = {}\n",
    "    self.root = -1\n",
    "    self.mut_cost_matrix = np.array([[0, 1, 1, 1], \n",
    "                                 [1, 0, 1, 1],\n",
    "                                 [1, 1, 0, 1],\n",
    "                                 [1, 1, 1, 0]]) \n",
    "    self.total_parsimony_score = 0\n",
    "    self.DNA_ind_map = {\"A\":0,\"T\":1,\"G\":2,\"C\":3}\n",
    "    self.DNA_list = \"ATGC\"\n",
    "    self.max_penalty = float(\"inf\")    \n",
    "    self.first_pos_of_unique_site_pattern_to_pos_list = {}\n",
    "    self.leaves = []\n",
    "    self.sequence_length = 0\n",
    "  def Add_mut_cost_matrix(self,W):\n",
    "    self.mut_cost_matrix = W  \n",
    "  def Add_vertex(self,name):    \n",
    "    v = Vertex(name)\n",
    "    self.vertex_map[name] = v  \n",
    "  def Contains_vertex(self,name):\n",
    "    return (name in self.vertex_map.keys())\n",
    "  def Get_vertex(self,name):\n",
    "    if name in self.vertex_map.keys():\n",
    "      return (self.vertex_map[name])\n",
    "\n",
    "  def Add_edge(self, end1_name, end2_name, distance):   #FOR PROJECT\n",
    "    if end1_name not in self.vertex_map.keys():\n",
    "      self.Add_vertex(end1_name)\n",
    "    if end2_name not in self.vertex_map.keys():\n",
    "      self.Add_vertex(end2_name)\n",
    "    node1 = self.Get_vertex(end1_name)\n",
    "    node2 = self.Get_vertex(end2_name)   \n",
    "    node1.neighbors.append(node2)\n",
    "    node2.neighbors.append(node1)\n",
    "    self.edge_list_map[(node1,node2)] = distance\n",
    "\n",
    "  def Add_directed_edge(self, parent_name, child_name, distance):   \n",
    "    if parent_name not in self.vertex_map.keys():\n",
    "      self.Add_vertex(parent_name)\n",
    "    if child_name not in self.vertex_map.keys():\n",
    "      self.Add_vertex(child_name)\n",
    "    p = self.Get_vertex(parent_name)\n",
    "    c = self.Get_vertex(child_name)    \n",
    "    p.out_degree += 1\n",
    "    c.in_degree += 1\n",
    "    c.parent = p\n",
    "    p.children.append(c)\n",
    "    self.edge_list_map[(p,c)] = distance\n",
    "  def Get_edge_length(self, parent, child):\n",
    "    return (self.edge_list_map[(parent, child)])\n",
    "  def Set_root(self):\n",
    "    for vertex in self.vertex_map.values():\n",
    "      if vertex.in_degree == 0:\n",
    "        self.root = vertex\n",
    "        self.root.is_root = True\n",
    "      else:\n",
    "        vertex.is_root = False\n",
    "  def Get_root(self):\n",
    "    if self.root == -1:\n",
    "      self.Set_root()\n",
    "    return (self.root)\n",
    "\n",
    "  def Set_MST_leaf_flags(self):   #FOR PROJECT\n",
    "    for v in self.vertex_map.values():\n",
    "      if len(v.neighbors) == 1:\n",
    "        v.is_leaf = True\n",
    "      else:\n",
    "        v.is_leaf = False\n",
    "        \n",
    "  def Set_leaf_flags(self):\n",
    "    for v in self.vertex_map.values():\n",
    "      if v.out_degree == 0:\n",
    "        v.is_leaf = True\n",
    "      else:\n",
    "        v.is_leaf = False\n",
    "  def Set_leaves(self):\n",
    "    self.leaves = []\n",
    "    for v in self.vertex_map.values():      \n",
    "      if v.is_leaf:\n",
    "        self.leaves.append(v)    \n",
    "  def Set_pre_order_and_post_order(self):\n",
    "    self.Set_root()\n",
    "    self.Set_leaf_flags()\n",
    "    self.Set_leaves()\n",
    "    self.pre_order_list = [self.root]\n",
    "    self.post_order_list = [self.root]\n",
    "    vertices_to_visit = [self.root]\n",
    "    while len(vertices_to_visit) > 0:\n",
    "      v = vertices_to_visit.pop()\n",
    "      vertices_to_visit += v.children      \n",
    "      self.post_order_list = v.children + self.post_order_list\n",
    "      self.pre_order_list = self.pre_order_list + v.children\n",
    "  def Compute_newick_format(self):\n",
    "    if len(self.post_order_list) != len(self.vertex_map):\n",
    "      self.Set_pre_order_and_post_order()\n",
    "    for v in self.post_order_list:\n",
    "      if v.out_degree == 0:\n",
    "        v.newick_label = v.name\n",
    "      else:        \n",
    "        c_l = v.children[0]\n",
    "        c_r = v.children[1]        \n",
    "        len_l = self.Get_edge_length(v,c_l)\n",
    "        len_r = self.Get_edge_length(v,c_r)\n",
    "        v.newick_label = f'({c_l.newick_label}:{len_l},{c_r.newick_label}:{len_r})'\n",
    "    self.root.newick_label += \";\"\n",
    "    return(self.root.newick_label)   \n",
    "\n",
    "  ################################################\n",
    "  # Add an iteration counter to avoind duplicated names for hidden vertices\n",
    "  #e.g., hidden_vertex_name = \"h\" + str(hidden_vertex_ind) + '_' + str(k)\n",
    "  ################################################\n",
    "  def Read_newick_string(self, newick_string, iteration_num):    \n",
    "    rx = r'\\([^()]+\\)'\n",
    "    hidden_vertex_ind = 1\n",
    "    while \",\" in newick_string:                  \n",
    "      # search for the parenthesis\n",
    "      m = re.search(rx,newick_string)\n",
    "      # returns a tuple containing all the subgroups of the match \"()\"\n",
    "      string_match = m.group()            \n",
    "      # remove ( and )\n",
    "      siblings_string = string_match[1:-1]      \n",
    "      c_left_name_and_length, c_right_name_and_length = siblings_string.split(\",\")\n",
    "      c_left_name, c_left_length = c_left_name_and_length.split(\":\")\n",
    "      c_right_name, c_right_length = c_right_name_and_length.split(\":\")\n",
    "      if not self.Contains_vertex(c_left_name):\n",
    "          self.Add_vertex(c_left_name)\n",
    "      if not self.Contains_vertex(c_right_name):\n",
    "          self.Add_vertex(c_right_name)\n",
    "      hidden_vertex_name = \"h\" + str(hidden_vertex_ind) + '_' + str(iteration_num)\n",
    "      self.Add_vertex(hidden_vertex_name)            \n",
    "      self.Add_directed_edge(hidden_vertex_name, c_left_name, float(c_left_length))\n",
    "      self.Add_directed_edge(hidden_vertex_name, c_right_name, float(c_right_length))\n",
    "      newick_string = newick_string.replace(string_match,hidden_vertex_name)\n",
    "      hidden_vertex_ind += 1 \n",
    "\n",
    "  def Read_newick_string_without_branch_lengths(self,newick_string, iteration_num):\n",
    "    rx = r'\\([^()]+\\)'\n",
    "    hidden_vertex_ind = 1\n",
    "    while \",\" in newick_string:                  \n",
    "      # search for the parenthesis\n",
    "      m = re.search(rx,newick_string)\n",
    "      # returns a tuple containing all the subgroups of the match \"()\"\n",
    "      string_match = m.group()            \n",
    "      # remove ( and )\n",
    "      siblings_string = string_match[1:-1]      \n",
    "      c_left_name, c_right_name = siblings_string.split(\",\")\n",
    "      # c_left_name, c_left_length = c_left_name_and_length.split(\":\")\n",
    "      # c_right_name, c_right_length = c_right_name_and_length.split(\":\")\n",
    "      if not self.Contains_vertex(c_left_name):\n",
    "          self.Add_vertex(c_left_name)\n",
    "      if not self.Contains_vertex(c_right_name):\n",
    "          self.Add_vertex(c_right_name)\n",
    "      hidden_vertex_name = \"h\" + str(hidden_vertex_ind) + '_' + str(iteration_num)\n",
    "      self.Add_vertex(hidden_vertex_name)            \n",
    "      self.Add_directed_edge(hidden_vertex_name, c_left_name, 0.001)\n",
    "      self.Add_directed_edge(hidden_vertex_name, c_right_name, 0.001)      \n",
    "      newick_string = newick_string.replace(string_match,hidden_vertex_name)\n",
    "      hidden_vertex_ind += 1 \n",
    "\n",
    "  # def Read_newick_string(self, newick_string):    \n",
    "  #   rx = r'\\([^()]+\\)'\n",
    "  #   hidden_vertex_ind = 1\n",
    "  #   while \",\" in newick_string:                  \n",
    "  #     # search for the parenthesis\n",
    "  #     m = re.search(rx,newick_string)\n",
    "  #     # returns a tuple containing all the subgroups of the match \"()\"\n",
    "  #     string_match = m.group()            \n",
    "  #     # remove ( and )\n",
    "  #     siblings_string = string_match[1:-1]      \n",
    "  #     c_left_name_and_length, c_right_name_and_length = siblings_string.split(\",\")\n",
    "  #     c_left_name, c_left_length = c_left_name_and_length.split(\":\")\n",
    "  #     c_right_name, c_right_length = c_right_name_and_length.split(\":\")\n",
    "  #     if not self.Contains_vertex(c_left_name):\n",
    "  #         self.Add_vertex(c_left_name)\n",
    "  #     if not self.Contains_vertex(c_right_name):\n",
    "  #         self.Add_vertex(c_right_name)\n",
    "  #     hidden_vertex_name = \"h\" + str(hidden_vertex_ind)\n",
    "  #     self.Add_vertex(hidden_vertex_name)            \n",
    "  #     self.Add_directed_edge(hidden_vertex_name, c_left_name, float(c_left_length))\n",
    "  #     self.Add_directed_edge(hidden_vertex_name, c_right_name, float(c_right_length))\n",
    "  #     newick_string = newick_string.replace(string_match,hidden_vertex_name)\n",
    "  #     hidden_vertex_ind += 1 \n",
    "\n",
    "\n",
    "  # def Read_newick_string_without_branch_lengths(self,newick_string):\n",
    "  #   rx = r'\\([^()]+\\)'\n",
    "  #   hidden_vertex_ind = 1\n",
    "  #   while \",\" in newick_string:                  \n",
    "  #     # search for the parenthesis\n",
    "  #     m = re.search(rx,newick_string)\n",
    "  #     # returns a tuple containing all the subgroups of the match \"()\"\n",
    "  #     string_match = m.group()            \n",
    "  #     # remove ( and )\n",
    "  #     siblings_string = string_match[1:-1]      \n",
    "  #     c_left_name, c_right_name = siblings_string.split(\",\")\n",
    "  #     # c_left_name, c_left_length = c_left_name_and_length.split(\":\")\n",
    "  #     # c_right_name, c_right_length = c_right_name_and_length.split(\":\")\n",
    "  #     if not self.Contains_vertex(c_left_name):\n",
    "  #         self.Add_vertex(c_left_name)\n",
    "  #     if not self.Contains_vertex(c_right_name):\n",
    "  #         self.Add_vertex(c_right_name)\n",
    "  #     hidden_vertex_name = \"h\" + str(hidden_vertex_ind)\n",
    "  #     self.Add_vertex(hidden_vertex_name)            \n",
    "  #     self.Add_directed_edge(hidden_vertex_name, c_left_name, 0.001)\n",
    "  #     self.Add_directed_edge(hidden_vertex_name, c_right_name, 0.001)      \n",
    "  #     newick_string = newick_string.replace(string_match,hidden_vertex_name)\n",
    "  #     hidden_vertex_ind += 1 \n",
    "\n",
    "\n",
    "  def Read_fasta_file(self,fasta_file_name):\n",
    "    fastaFile = open(fasta_file_name,'r')\n",
    "    seq = ''\n",
    "    name = ''\n",
    "    sequenceAlignment={}\n",
    "    for line in fastaFile:\n",
    "        if line.startswith('>'):\n",
    "            if seq != '':\n",
    "                seq = seq.upper()\n",
    "                if not self.Contains_vertex(name):\n",
    "                  self.Add_vertex(name)\n",
    "                v = self.Get_vertex(name)\n",
    "                v.sequence = seq       \n",
    "                seq = ''\n",
    "            name = line.strip().split('>')[1]\n",
    "        else:\n",
    "            seq += line.strip()\n",
    "    if not self.Contains_vertex(name):\n",
    "      self.Add_vertex(name)\n",
    "    v = self.Get_vertex(name)\n",
    "    v.sequence = seq    \n",
    "    self.sequence_length = len(seq)\n",
    "    # sequenceAlignment[name] = seq\n",
    "    fastaFile.close()\n",
    "  def Set_null_sequences_in_ancestors(self):\n",
    "    for v in self.vertex_map.values():\n",
    "      if not v.is_leaf:\n",
    "        v.sequence = \"N\"*self.sequence_length\n",
    "  def Get_site_pattern(self,site):\n",
    "    site_pattern = \"\".join([l.sequence[site] for l in self.leaves])\n",
    "    return(site_pattern)\n",
    "  def Store_pos_list_for_unique_site_patterns(self):\n",
    "    self.Set_pre_order_and_post_order()\n",
    "    self.first_pos_of_unique_site_pattern_to_pos_list = {}\n",
    "    unique_site_pattern_to_first_pos = {}\n",
    "    for site in range(self.sequence_length):\n",
    "      site_pattern = \"\".join([l.sequence[site] for l in self.leaves])\n",
    "      # print(f\"site pattern for pos{site} is {site_pattern}\")\n",
    "      if site_pattern in unique_site_pattern_to_first_pos.keys():                \n",
    "        first_pos = unique_site_pattern_to_first_pos[site_pattern] \n",
    "        self.first_pos_of_unique_site_pattern_to_pos_list[first_pos].append(site)\n",
    "      else:\n",
    "        unique_site_pattern_to_first_pos[site_pattern] = site\n",
    "        self.first_pos_of_unique_site_pattern_to_pos_list[site] = [site]    \n",
    "  def Is_informative(self,site_pattern):\n",
    "    counts = Counter(site_pattern)\n",
    "    return(sum([count > 1 for count in counts.values()])>1)\n",
    "  def Get_total_number_of_informative_site_patterns(self):\n",
    "    total_number_of_informative_sites = 0\n",
    "    total_number_of_informative_site_patterns = 0\n",
    "    for site in self.first_pos_of_unique_site_pattern_to_pos_list.keys():     \n",
    "      site_pattern = \"\".join([l.sequence[site] for l in self.leaves])  \n",
    "      if (self.Is_informative(site_pattern)):\n",
    "        pattern_weight = len(self.first_pos_of_unique_site_pattern_to_pos_list[site])        \n",
    "        total_number_of_informative_sites += pattern_weight\n",
    "        total_number_of_informative_site_patterns += 1\n",
    "    print(f\"total number of informative sites is {total_number_of_informative_sites}\")\n",
    "    print(f\"total number of unique informative_site_patterns is {total_number_of_informative_site_patterns}\")\n",
    "  def Run_Sankoff(self,site):     \n",
    "    # for site in range(self.self.sequence_length):\n",
    "    # Phase 1 compute minimum mutation cost for subtrees\n",
    "    pattern_weight = len(self.first_pos_of_unique_site_pattern_to_pos_list[site])\n",
    "    pos_list = self.first_pos_of_unique_site_pattern_to_pos_list[site]\n",
    "    for parent in self.post_order_list:\n",
    "      if parent.is_leaf:\n",
    "        parent.min_cost_subtree = [self.max_penalty] * 4\n",
    "        dna_p_ind = self.DNA_ind_map[parent.sequence[site]]\n",
    "        parent.min_cost_subtree[dna_p_ind] = 0        \n",
    "      else:\n",
    "        parent.min_cost_subtree = [0] * 4\n",
    "        for dna_p_ind in range(4):\n",
    "          for child in parent.children:                        \n",
    "            min_cost_from_child_subtree = self.max_penalty\n",
    "            for dna_c_ind in range(4):\n",
    "              min_cost_from_child_subtree = min(min_cost_from_child_subtree, self.mut_cost_matrix[dna_p_ind][dna_c_ind] + child.min_cost_subtree[dna_c_ind])          \n",
    "            parent.min_cost_subtree[dna_p_ind] += min_cost_from_child_subtree            \n",
    "    # Phase 2 compute ancestral sequences\n",
    "    min_cost_ind = self.root.min_cost_subtree.index(min(self.root.min_cost_subtree))\n",
    "    self.total_parsimony_score += self.root.min_cost_subtree[min_cost_ind] * pattern_weight    \n",
    "    dna_root = self.DNA_list[min_cost_ind]\n",
    "    for pos in self.first_pos_of_unique_site_pattern_to_pos_list[site]:      \n",
    "      self.root.sequence = self.root.sequence[:pos] + dna_root + self.root.sequence[(pos+1):]\n",
    "    for child in self.pre_order_list[1:]:\n",
    "      if not child.is_leaf:\n",
    "        parent = child.parent\n",
    "        char_parent = parent.sequence[site]\n",
    "        dna_p_ind = self.DNA_ind_map[char_parent]\n",
    "        assert(char_parent != \"N\")\n",
    "        min_cost_ind = 0\n",
    "        min_cost = self.max_penalty\n",
    "        for dna_c_ind in range(4):\n",
    "          if self.mut_cost_matrix[dna_p_ind][dna_c_ind] + child.min_cost_subtree[dna_c_ind] < min_cost:\n",
    "            min_cost = self.mut_cost_matrix[dna_p_ind][dna_c_ind] + child.min_cost_subtree[dna_c_ind]\n",
    "            min_cost_ind = dna_c_ind\n",
    "        dna_child = self.DNA_list[min_cost_ind]\n",
    "        for pos in self.first_pos_of_unique_site_pattern_to_pos_list[site]:\n",
    "          child.sequence = child.sequence[:pos] + dna_child + child.sequence[(pos+1):]    \n",
    "  def Run_Sankoff_for_all_sites(self):\n",
    "    self.Set_pre_order_and_post_order()\n",
    "    self.Set_null_sequences_in_ancestors()\n",
    "    for site in self.first_pos_of_unique_site_pattern_to_pos_list.keys():\n",
    "      site_pattern = \"\".join([l.sequence[site] for l in self.leaves])\n",
    "      self.Run_Sankoff(site)\n",
    "  def Suppress_the_root(self):\n",
    "    root = self.Get_root()\n",
    "    self.Set_neighbors(root)\n",
    "    del self.vertex_map[root.name]\n",
    "    for v in self.vertex_map.values():\n",
    "      v.parent = -1\n",
    "      v.children = []\n",
    "  def Set_neighbors(self, vertex):\n",
    "    vertex.neighbors.extend(vertex.children)\n",
    "    for c in vertex.children:\n",
    "      if vertex.is_root:\n",
    "        # c.neighbors.extend(list(set(vertex.children) - set(c)))\n",
    "        c.neighbors.extend([item for item in vertex.children if item.name != c.name])\n",
    "        self.Set_neighbors(c)\n",
    "      else:\n",
    "        c.neighbors.append(vertex)\n",
    "        self.Set_neighbors(c)\n",
    "\n",
    "  def Root_tree_along_branch(self, node1_name, node2_name):\n",
    "    self.Add_vertex(\"root\")\n",
    "    self.Set_root()\n",
    "    self.root.out_degree = 2\n",
    "    vertices_to_visit = [self.root]\n",
    "    while len(vertices_to_visit) > 0:\n",
    "      v = vertices_to_visit.pop()\n",
    "      if v.is_root:\n",
    "        v.children = [self.Get_vertex(node1_name), self.Get_vertex(node2_name)]\n",
    "        for child in v.children:\n",
    "            child.parent = v\n",
    "\n",
    "        vertices_to_visit += v.children\n",
    "      elif v.name == node1_name:\n",
    "        if v.is_leaf:\n",
    "          v.neighbors = []\n",
    "          continue\n",
    "        else:\n",
    "          v.children = v.neighbors\n",
    "          v.children.remove(self.Get_vertex(node2_name))\n",
    "          for child in v.children:\n",
    "            child.parent = v\n",
    "          v.neighbors = []\n",
    "\n",
    "          vertices_to_visit += v.children\n",
    "      elif v.name == node2_name:\n",
    "        if v.is_leaf:\n",
    "          v.neighbors = []\n",
    "          continue\n",
    "        else:\n",
    "          v.children = v.neighbors\n",
    "          v.children.remove(self.Get_vertex(node1_name))\n",
    "          for child in v.children:\n",
    "            child.parent = v\n",
    "          v.neighbors = []\n",
    "\n",
    "          vertices_to_visit += v.children\n",
    "      else:\n",
    "        if v.is_leaf:\n",
    "          v.neighbors = []\n",
    "          continue\n",
    "        else:\n",
    "          v.children = v.neighbors\n",
    "          v.children.remove(v.parent)\n",
    "          for child in v.children:\n",
    "            child.parent = v\n",
    "          v.neighbors = []\n",
    "\n",
    "          vertices_to_visit += v.children\n",
    "    self.total_parsimony_score = 0\n",
    "    # for v in T.vertex_map.values():\n",
    "    #   v.neighbors = []\n",
    "    \n",
    "\n",
    "  def NNI(self, node1_name, node2_name):\n",
    "    T1 = copy.deepcopy(self)\n",
    "\n",
    "    node1_subtrees = T1.Get_vertex(node1_name).neighbors\n",
    "    # print([v.name for v in node1_subtrees])\n",
    "    node1_subtrees.remove(T1.Get_vertex(node2_name))\n",
    "    subtree_1 = node1_subtrees[0]\n",
    "    subtree_2 = node1_subtrees[1]\n",
    "\n",
    "    node2_subtrees = T1.Get_vertex(node2_name).neighbors\n",
    "    node2_subtrees.remove(T1.Get_vertex(node1_name))\n",
    "    subtree_3 = node2_subtrees[0]\n",
    "    subtree_4 = node2_subtrees[1]\n",
    "   \n",
    "    T1.Get_vertex(node1_name).neighbors = [subtree_1, subtree_3, T1.Get_vertex(node2_name)]\n",
    "    T1.Get_vertex(node2_name).neighbors = [subtree_4, subtree_2, T1.Get_vertex(node1_name)]\n",
    "    subtree_2.neighbors.remove(T1.Get_vertex(node1_name))\n",
    "    subtree_2.neighbors.append(T1.Get_vertex(node2_name))\n",
    "    subtree_3.neighbors.remove(T1.Get_vertex(node2_name))\n",
    "    subtree_3.neighbors.append(T1.Get_vertex(node1_name))\n",
    "\n",
    "    T2 = copy.deepcopy(self)\n",
    "\n",
    "    node1_subtrees = T2.Get_vertex(node1_name).neighbors\n",
    "    node1_subtrees.remove(T2.Get_vertex(node2_name))\n",
    "    subtree_1 = node1_subtrees[0]\n",
    "    subtree_2 = node1_subtrees[1]\n",
    "\n",
    "    node2_subtrees = T2.Get_vertex(node2_name).neighbors\n",
    "    node2_subtrees.remove(T2.Get_vertex(node1_name))\n",
    "    subtree_3 = node2_subtrees[0]\n",
    "    subtree_4 = node2_subtrees[1]\n",
    "\n",
    "    T2.Get_vertex(node1_name).neighbors = [subtree_1, subtree_4, T2.Get_vertex(node2_name)]\n",
    "    T2.Get_vertex(node2_name).neighbors = [subtree_3, subtree_2, T2.Get_vertex(node1_name)]\n",
    "    subtree_2.neighbors.remove(T2.Get_vertex(node1_name))\n",
    "    subtree_2.neighbors.append(T2.Get_vertex(node2_name))\n",
    "    subtree_4.neighbors.remove(T2.Get_vertex(node2_name))\n",
    "    subtree_4.neighbors.append(T2.Get_vertex(node1_name))\n",
    "\n",
    "    return T1, T2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pvt4cTsHnk4"
   },
   "source": [
    "#2. MST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0-0YjDkHHspI"
   },
   "outputs": [],
   "source": [
    "#I. Computing pairwise Jukes-Cantor distances between sequences\n",
    "def jukes_cantor_distance(sequence1, sequence2):\n",
    "  # print('computing JC distance')\n",
    "  nucleotide_diff_count = 0 # how many nucleotides they differ\n",
    "  total_nucleotides_compared = 0.0\n",
    "  bases = {'A', 'C', 'G', 'T'}\n",
    "  \n",
    "  for a, b in zip(sequence1, sequence2):\n",
    "    if a in bases and b in bases:\n",
    "      # total_nucleotides_compared += 1\n",
    "      if a != b: \n",
    "        nucleotide_diff_count += 1\n",
    "      \n",
    "  nucleotide_diff_percentage = nucleotide_diff_count / len(sequence1)\n",
    "  # diff_pct_adjusted = min(nucleotide_diff_percentage, 0.74999)\n",
    "  # jukes_cantor_distance = -0.75 * math.log(1 - min((diff_pct_adjusted*(4.0/3.0)),1.0))\n",
    "  jukes_cantor_distance = -0.75 * math.log(1 - (nucleotide_diff_percentage*(4.0/3.0)))\n",
    "  return jukes_cantor_distance\n",
    "\n",
    "#II. Creating a matrix of pairwise Jukes-Cantor distance\n",
    "def create_distance_matrix(seq_dict):\n",
    "  dist_mat_new = np.zeros((len(seq_dict), len(seq_dict)))\n",
    "  \n",
    "  for i in range(len(seq_dict)):\n",
    "    for j in range(i+1, len(seq_dict)):\n",
    "      dist_mat_new[i][j] = jukes_cantor_distance(seq_dict[list(seq_dict.keys())[i]], seq_dict[list(seq_dict.keys())[j]])\n",
    "  return dist_mat_new\n",
    "\n",
    "#III. Creating networkx-based MST with distance matrix\n",
    "def create_mst_from_dist_mat(seq_dict, distance_matrix):\n",
    "  seq_id_new = list(seq_dict.keys())\n",
    "  G_new = nx.Graph()\n",
    "  G_new.add_nodes_from(seq_id_new)\n",
    "  for i in range(len(seq_id_new)):\n",
    "    for j in range(i+1, len(seq_id_new)):\n",
    "      G_new.add_edge(seq_id_new[i], seq_id_new[j], weight = distance_matrix[i][j])\n",
    "  MST = nx.minimum_spanning_tree(G_new)\n",
    "  return MST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su35HDa5HuKb"
   },
   "source": [
    "#3. Inferring local tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A3yAzgydH0XQ"
   },
   "outputs": [],
   "source": [
    "#I. Selecting Vs from the MST such that it induces a subtree\n",
    "def find_subtree(T, threshold): # e.g. threshold = 10\n",
    "  # initialize vertices_to_visit to all of the leaves\n",
    "  vertices_to_visit = T.leaves\n",
    "  while len(vertices_to_visit) > 0:\n",
    "    v = vertices_to_visit.pop(0)\n",
    "  \n",
    "    if v.is_leaf:\n",
    "        v.subtree.append(v)\n",
    "        if v.neighbors[0] not in vertices_to_visit:\n",
    "          vertices_to_visit.extend(v.neighbors)\n",
    "    else:\n",
    "    #if v is an internal vertex, \n",
    "      #check the number of neighbors with empty subtrees\n",
    "      num_neighbor_with_empty_subtree = sum([len(n.subtree) == 0 for n in v.neighbors])\n",
    "      # print(num_neighbor_with_empty_subtree)\n",
    "\n",
    "      #if only one of its neighbors has an empty(unassigned) subtree, assign the subtree at this node\n",
    "      if num_neighbor_with_empty_subtree == 1:\n",
    "        # print(v.name)\n",
    "        v.subtree.append(v)\n",
    "        for n in v.neighbors:\n",
    "          if len(n.subtree) > 0:\n",
    "            for subtree_vertex in n.subtree:\n",
    "              if subtree_vertex not in v.subtree:\n",
    "                v.subtree.append(subtree_vertex)\n",
    "          elif n not in vertices_to_visit:\n",
    "            vertices_to_visit.append(n)\n",
    "\n",
    "      #if the number of neighbors with empty(unassigned) subtrees > 1, \n",
    "      #then queue this vertex for re-visiting after updating its neighbors\n",
    "      else:\n",
    "        for n in v.neighbors:\n",
    "          if len(n.subtree) == 0 and n not in vertices_to_visit:\n",
    "            vertices_to_visit.append(n)\n",
    "        vertices_to_visit.append(v)\n",
    " \n",
    "    if len(v.subtree) >= threshold: \n",
    "      return v.subtree\n",
    "\n",
    "#II. Selecting Ve by performing BFS from the root of the induced subtree from Vs \n",
    "#input: MST tree, Vs, threshold for Ve which is consistent with Vs\n",
    "#output: Ve consisting of neighboring vertices of Vs\n",
    "def extra_vertices_with_bfs(MST, Vs, threshold): #function for BFS\n",
    "  Ve = [] \n",
    "  visited = []  #List for visited nodes.\n",
    "  queue = []     #Initialize a queue\n",
    "\n",
    "  visited.append(Vs[0])\n",
    "  queue.append(Vs[0])\n",
    "\n",
    "  while queue:          # Creating loop to visit each node\n",
    "    m = queue.pop(0) \n",
    "    \n",
    "    for neighbor in MST.Get_vertex(m.name).neighbors:\n",
    "      if neighbor not in visited:\n",
    "        visited.append(neighbor)\n",
    "        queue.append(neighbor)\n",
    "        if neighbor not in Ve and neighbor not in Vs: #vertices in Vs should not be in Ve\n",
    "          Ve.append(neighbor)\n",
    "      if len(Ve) >= threshold:\n",
    "        return Ve\n",
    "\n",
    "#III. Finding all internal edges for NNI\n",
    "def find_internal_edge(T):\n",
    "  internal_vertex_names = [v.name for v in T.vertex_map.values() if v.is_leaf == False]\n",
    "  internal_edge_possibilities = list(itertools.combinations(internal_vertex_names,2))\n",
    "  internal_edges = [p for p in internal_edge_possibilities if T.Get_vertex(p[0]) in T.Get_vertex(p[1]).neighbors]\n",
    "  return internal_edges\n",
    "\n",
    "#IV. Searching for all possible tree topologies with NNI\n",
    "#input: unrooted tree\n",
    "def search_tree(tree, root_branch_node1, root_branch_node2, nni_node1, nni_node2):\n",
    "  # 1. COMPUTE A TREE\n",
    "  # root the tree at an arbitrary branch\n",
    "  tree.Root_tree_along_branch(root_branch_node1, root_branch_node2)\n",
    "\n",
    "  # 2. COMPUTE THE PARSIMONY SCORE (of the rooted tree)\n",
    "  tree.Run_Sankoff_for_all_sites()\n",
    "  total_parsimony_score = tree.total_parsimony_score\n",
    "  # print(f\"Total parsimony score is {tree.total_parsimony_score}\")\n",
    "\n",
    "  # 3. MODIFY TREE TOPOLOGY\n",
    "  tree.Suppress_the_root()\n",
    "  T1, T2 = tree.NNI(nni_node1, nni_node2)\n",
    "\n",
    "  T1.Root_tree_along_branch(nni_node1, nni_node2)\n",
    "  T2.Root_tree_along_branch(nni_node1, nni_node2)\n",
    "  T1.Run_Sankoff_for_all_sites()\n",
    "  T2.Run_Sankoff_for_all_sites()\n",
    "  T1_parsimony_score = T1.total_parsimony_score\n",
    "  T2_parsimony_score = T2.total_parsimony_score\n",
    "  # print(f\"T1 parsimony score: {T1_parsimony_score}\")\n",
    "  # print(f\"T2 parsimony score: {T2_parsimony_score}\")\n",
    "\n",
    "  if T1_parsimony_score < total_parsimony_score and T1_parsimony_score < T2_parsimony_score:\n",
    "    # print(\"changing to T1\")\n",
    "    T1.Suppress_the_root()\n",
    "    return T1\n",
    "  elif T2_parsimony_score < total_parsimony_score and T2_parsimony_score < T1_parsimony_score:\n",
    "    # print(\"changing to T2\")\n",
    "    T2.Suppress_the_root()\n",
    "    return T2\n",
    "  else:\n",
    "    # print(\"not changing tree topology\")\n",
    "    return tree   \n",
    "\n",
    "# TODO: find function from package that takes sequences as input and output newick format of MP tree\n",
    "\n",
    "#IV. Finding the local tree with maximum parsimony score\n",
    "def infer_MP_local_tree(V, sequences, iteration_counter, cost_matrix):\n",
    "\n",
    "  final_lowest_parsimony_score = float('inf')\n",
    "  k = 1\n",
    "\n",
    "  #starting with 3 random initial tree topologies to avoid local optimum\n",
    "  while k < 4:\n",
    "    print(f'##### Infer MP local tree: interation {k} begins #####')\n",
    "    t = ete3.Tree()\n",
    "    t.populate(len(V), [v.name for v in V])\n",
    "    newick_str = t.write(format = 9)\n",
    "    \n",
    "    \n",
    "    T = Tree('local')\n",
    "    T.Read_newick_string_without_branch_lengths(newick_str, iteration_counter)\n",
    "    for v in V:\n",
    "      T.Get_vertex(v.name).sequence = sequences[v.name]\n",
    "      #print(T.Get_vertex(v.name).sequence)\n",
    "    T.sequence_length = len(sequences[V[0].name])\n",
    "    # for l in T.leaves:\n",
    "    #   print(l.sequence)\n",
    "\n",
    "    # for v in T.vertex_map.values():\n",
    "    #   print(f'vertex {v.name}: out-degree {v.out_degree}, neighbors {[n.name for n in v.neighbors]}, parent {v.parent.name}, children {[c.name for c in v.children]}')\n",
    "\n",
    "    T.Add_mut_cost_matrix(cost_matrix)\n",
    "    T.Store_pos_list_for_unique_site_patterns()\n",
    "    print(f\"Total number of unique site patterns is {len(T.first_pos_of_unique_site_pattern_to_pos_list)}\")\n",
    "    T.Get_total_number_of_informative_site_patterns()\n",
    "    T.Run_Sankoff_for_all_sites()\n",
    "    total_parsimony_score = T.total_parsimony_score\n",
    "    print(f\"Total parsimony score is {T.total_parsimony_score}\")\n",
    "    T.Suppress_the_root()\n",
    "\n",
    "    if k == 1:\n",
    "      T_final = T\n",
    "\n",
    "    T_lowest = T\n",
    "    cur_lowest_parsimony_score = float('inf')\n",
    "    continue_search = True\n",
    "\n",
    "    while(continue_search):\n",
    "      # print(\"one more while iteration\")\n",
    "      T_new = T_lowest\n",
    "      continue_search = False\n",
    "      internal_edges = find_internal_edge(T_lowest)\n",
    "      \n",
    "\n",
    "      for b in internal_edges:\n",
    "        # print(f'NNI branch: ({b[0]}, {b[1]})')\n",
    "        tree = search_tree(T_new, b[0], b[1], b[0], b[1])\n",
    "        if tree.total_parsimony_score < cur_lowest_parsimony_score:\n",
    "          # print(\"Update continue_search to TRUE\")\n",
    "          cur_lowest_parsimony_score = tree.total_parsimony_score\n",
    "          T_lowest = tree\n",
    "          continue_search = True\n",
    "    print(f'For iteration {k}, best parsimony score: {cur_lowest_parsimony_score}')\n",
    "\n",
    "    if cur_lowest_parsimony_score < final_lowest_parsimony_score:\n",
    "      T_final = T_lowest\n",
    "      final_lowest_parsimony_score = cur_lowest_parsimony_score\n",
    "\n",
    "    k += 1\n",
    "\n",
    "  return T_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8tdIKsQH2bR"
   },
   "source": [
    "#4. Infering global tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "s5F2jvP7H4i8"
   },
   "outputs": [],
   "source": [
    "#I. Finding the subtree induced by only Vs in the local tree\n",
    "#input: local unrooted phylogeny tree, Vs\n",
    "#output: dictionary with a tuple of two child nodes from Vs as key and the corresponding hidden vertex as value \n",
    "#e.g.,    A\n",
    "#        / \\\n",
    "#       B   C\n",
    "def informative_subtree(tree, Vs):\n",
    "  forest = {}\n",
    "  for v_tuple in itertools.combinations(Vs, 2):\n",
    "    if tree.Get_vertex(v_tuple[0].name).neighbors[0] is tree.Get_vertex(v_tuple[1].name).neighbors[0]:\n",
    "      forest[v_tuple] = tree.Get_vertex(v_tuple[0].name).neighbors[0]\n",
    "  \n",
    "  return forest\n",
    "\n",
    "#II. Deleting paired Vs vertices and Adding the new parent vertices to V\n",
    "def update_subtree_vertices(forest, V):\n",
    "  V_new = V.copy()\n",
    "  \n",
    "  for children, parent_new in forest.items():\n",
    "    if children[0] in V:\n",
    "      V_new.remove(children[0])\n",
    "      # print(f'removed {children[0].name} from V_new')\n",
    "    if children[1] in V:\n",
    "      V_new.remove(children[1])\n",
    "      # print(f'removed {children[1].name} from V_new')\n",
    "    V_new.append(parent_new)\n",
    "    # print(f'added {parent_new.name} to V_new')\n",
    "  return V_new\n",
    "\n",
    "#III. Deleting paired Vs vertices and Adding the new parent vertices to the sequence dictionary\n",
    "def update_sequences(forest, seq_dict):\n",
    "  sequences_new = seq_dict.copy()\n",
    "  for children, parent_new in forest.items():\n",
    "    if children[0].name in seq_dict.keys():\n",
    "      del sequences_new[children[0].name]\n",
    "      # print(f'removed {children[0].name} from sequences_new')\n",
    "    if children[1].name in seq_dict.keys():\n",
    "      del sequences_new[children[1].name]\n",
    "      # print(f'removed {children[1].name} from sequences_new')\n",
    "    sequences_new[parent_new.name] = parent_new.sequence\n",
    "    # print(f'added {parent_new.name} to sequences_new')\n",
    "  return sequences_new\n",
    "\n",
    "\n",
    "#IV. Updating MST\n",
    "def update_mst(MST, forest, V_old, sequences_sub, dist_mat_sub):\n",
    "  MST_copy = MST.copy()\n",
    "  #delete all within-subgraph edge from MST\n",
    "  M_sub = nx.subgraph(MST, [v.name for v in V_old])\n",
    "  for edge in list(M_sub.edges):\n",
    "    MST_copy.remove_edge(edge[0], edge[1])\n",
    "  #delete paired $V_s$ vertices and add new parent vertices to MST\n",
    "  for children, parent_new in forest.items():\n",
    "      if children[0].name in MST_copy.nodes:\n",
    "        MST_copy.remove_node(children[0].name)\n",
    "        # print(f'removed {children[0].name} from MST')\n",
    "      if children[1].name in MST_copy.nodes:\n",
    "        MST_copy.remove_node(children[1].name)\n",
    "        # print(f'removed {children[1].name} from MST')\n",
    "      MST_copy.add_node(parent_new.name)\n",
    "      # print(f'added {parent_new.name} to MST')\n",
    "  #create the sub-MST of V\n",
    "  MST_sub = create_mst_from_dist_mat(sequences_sub, dist_mat_sub)\n",
    "  #add edges in sub-MST to MST\n",
    "  for edge in list(MST_sub.edges):\n",
    "    MST_copy.add_edge(edge[0], edge[1], weight = MST_sub.get_edge_data(edge[0], edge[1])['weight'])\n",
    "\n",
    "  return MST_copy  \n",
    "\n",
    "#V. Inferring the global phylogeny tree\n",
    "def infer_global_tree(T_global, MST, sequences, cost_matrix, threshold):\n",
    "  #k is an int as the iteration counter to avoid duplicated naming for hidden vertex\n",
    "  k = 1\n",
    "  #is_last is a boolean variable indicating whether this is the last iteration\n",
    "  is_last = False\n",
    "  #num_sequences is an int storing the initial number of sequences (taxons)\n",
    "  num_sequences = len(MST.nodes)\n",
    "  #plot_1 is a boolean variable indicating whether to plot MST when number of nodes drops to 2/3 of the initial number for the first time\n",
    "  plot_1 = True\n",
    "  #plot_2 is a boolean variable indicating whether to plot MST when number of nodes drops to 1/3 of the initial number for the first time\n",
    "  plot_2 = True\n",
    "\n",
    "  while len(T_global.edge_list_map) < len(T_global.vertex_map) - 1:\n",
    "    print(f'#################### Infer global tree: iteration {k} begins ####################')\n",
    "\n",
    "    #Converting networkx object to our own Tree class\n",
    "    M = Tree('MST')\n",
    "    for edge_tuple in MST.edges():\n",
    "      M.Add_edge(edge_tuple[0], edge_tuple[1], MST.get_edge_data(edge_tuple[0], edge_tuple[1])['weight'])\n",
    "    M.Set_MST_leaf_flags()\n",
    "    M.Set_leaves()\n",
    "    for vertex_name in M.vertex_map.keys():\n",
    "      M.Get_vertex(vertex_name).subtree = []\n",
    "\n",
    "    #if number of vertices in MST is smaller than threshold,\n",
    "    if len(M.vertex_map) <= threshold:\n",
    "      #then all vertices will be used to constitute the last local tree\n",
    "      V = [i for i in M.vertex_map.values()]\n",
    "      is_last = True\n",
    "    \n",
    "    else:\n",
    "      #Selecting Vs from the MST such that it induces a subtree\n",
    "      Vs = find_subtree(M, threshold)\n",
    "      #if |Vm| - |Vs| > threshold,\n",
    "      if(len(M.vertex_map) - len(Vs) > threshold):\n",
    "        #Selecting Ve by performing BFS from the root of the induced subtree from Vs\n",
    "        Ve = Ve = extra_vertices_with_bfs(M, Vs, 10)\n",
    "        V = Vs + Ve\n",
    "      else:\n",
    "        V = [i for i in M.vertex_map.values()]\n",
    "        is_last = True\n",
    "\n",
    "    T_lowest = infer_MP_local_tree(V, sequences, k, cost_matrix)\n",
    "\n",
    "    #if this is the last iteration, \n",
    "    if is_last:\n",
    "      #add all edges in the local tree to the global tree\n",
    "      vertices_visited = []\n",
    "      for v_name, v in T_lowest.vertex_map.items():\n",
    "        vertices_visited.append(v_name)\n",
    "        for neighbor in v.neighbors:\n",
    "          if(neighbor.name not in vertices_visited):\n",
    "            T_global.Add_edge(v_name, neighbor.name, 0.1)\n",
    "            print(f'Updating global tree: Added edge from {v_name} to {neighbor.name}, distance = 0.1')\n",
    "\n",
    "      \n",
    "    else: \n",
    "      #Finding the subtree induced by only Vs in the local tree\n",
    "      forest = informative_subtree(T_lowest, Vs)\n",
    "\n",
    "      #if forest is not empty, \n",
    "      if (len(forest) > 0):\n",
    "\n",
    "        #Updating the global phylogeny tree\n",
    "        for children, parent_new in forest.items():\n",
    "          T_global.Add_edge(children[0].name, parent_new.name, 0.1)\n",
    "          T_global.Add_edge(children[1].name, parent_new.name, 0.1)\n",
    "          print(f'Updating global tree: Added edge from {children[0].name} to {parent_new.name}, distance = 0.1')\n",
    "          print(f'Updating global tree: Added edge from {children[1].name} to {parent_new.name}, distance = 0.1')\n",
    "        \n",
    "        #Deleting paired Vs vertices and Adding the new parent vertices to V\n",
    "        V_new = update_subtree_vertices(forest, V)\n",
    "\n",
    "        #Deleting paired Vs vertices and Adding the new parent vertices to the sequence dictionary\n",
    "        sequences = update_sequences(forest, sequences)\n",
    "        \n",
    "        #Updating the subgraph distance matrix of V_new\n",
    "        sequences_sub = {k: sequences[k] for k in [v.name for v in V_new]}\n",
    "        dist_mat_sub = create_distance_matrix(sequences_sub)\n",
    "        \n",
    "        #Updating the edges in the subgraph consisting of V_new in MST\n",
    "        MST = update_mst(MST, forest, V, sequences_sub, dist_mat_sub)\n",
    "\n",
    "      print(f'Current MST has {len(MST.nodes)} nodes')\n",
    "      if plot_1 and len(MST.nodes) <= (2/3)*num_sequences:\n",
    "        fig, ax = plt.subplots(figsize=(30, 30))\n",
    "        nx.draw(MST, with_labels=True, font_weight='bold', node_size = 500)\n",
    "        fig.savefig(f'MST_{len(MST.nodes)}.png')\n",
    "        plot_1 = False\n",
    "      if plot_2 and len(MST.nodes) <= (1/3)*num_sequences:\n",
    "        fig, ax = plt.subplots(figsize=(30, 30))\n",
    "        nx.draw(MST, with_labels=True, font_weight='bold', node_size = 500)\n",
    "        fig.savefig(f'MST_{len(MST.nodes)}.png')\n",
    "        plot_2 = False\n",
    "\n",
    "      print(f'Current global tree has {len(T_global.vertex_map)} vertices and {len(T_global.edge_list_map)} edges')\n",
    "    \n",
    "    \n",
    "\n",
    "    df.loc[k] = [k, T_lowest.total_parsimony_score, len(MST.nodes), len(T_global.vertex_map), len(T_global.edge_list_map), len(V)]\n",
    "\n",
    "    k += 1\n",
    "  \n",
    "  return T_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_5u_xSrH618"
   },
   "source": [
    "#5. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64xvprrYH848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Infer global tree: iteration 1 begins ####################\n",
      "##### Infer MP local tree: interation 1 begins #####\n",
      "Total number of unique site patterns is 107\n",
      "total number of informative sites is 88\n",
      "total number of unique informative_site_patterns is 59\n",
      "Total parsimony score is 299\n",
      "For iteration 1, best parsimony score: 229\n",
      "##### Infer MP local tree: interation 2 begins #####\n",
      "Total number of unique site patterns is 107\n",
      "total number of informative sites is 88\n",
      "total number of unique informative_site_patterns is 59\n",
      "Total parsimony score is 303\n",
      "For iteration 2, best parsimony score: 219\n",
      "##### Infer MP local tree: interation 3 begins #####\n",
      "Total number of unique site patterns is 107\n",
      "total number of informative sites is 88\n",
      "total number of unique informative_site_patterns is 59\n",
      "Total parsimony score is 268\n",
      "For iteration 3, best parsimony score: 236\n",
      "Updating global tree: Added edge from EPI_ISL_146765_1968 to h15_1, distance = 0.1\n",
      "Updating global tree: Added edge from EPI_ISL_110747_1969 to h15_1, distance = 0.1\n",
      "Updating global tree: Added edge from EPI_ISL_113035_1969 to h11_1, distance = 0.1\n",
      "Updating global tree: Added edge from EPI_ISL_110777_1970 to h11_1, distance = 0.1\n",
      "Updating global tree: Added edge from EPI_ISL_110779_1971 to h13_1, distance = 0.1\n",
      "Updating global tree: Added edge from EPI_ISL_8676_1972 to h13_1, distance = 0.1\n",
      "Current MST has 95 nodes\n",
      "Current global tree has 101 vertices and 6 edges\n",
      "#################### Infer global tree: iteration 2 begins ####################\n",
      "##### Infer MP local tree: interation 1 begins #####\n",
      "Total number of unique site patterns is 106\n",
      "total number of informative sites is 89\n",
      "total number of unique informative_site_patterns is 59\n",
      "Total parsimony score is 290\n",
      "For iteration 1, best parsimony score: 256\n",
      "##### Infer MP local tree: interation 2 begins #####\n",
      "Total number of unique site patterns is 106\n",
      "total number of informative sites is 89\n",
      "total number of unique informative_site_patterns is 59\n",
      "Total parsimony score is 327\n",
      "For iteration 2, best parsimony score: 267\n",
      "##### Infer MP local tree: interation 3 begins #####\n",
      "Total number of unique site patterns is 106\n",
      "total number of informative sites is 89\n",
      "total number of unique informative_site_patterns is 59\n",
      "Total parsimony score is 318\n",
      "For iteration 3, best parsimony score: 195\n",
      "Updating global tree: Added edge from h15_1 to h8_2, distance = 0.1\n",
      "Updating global tree: Added edge from EPI_ISL_22614_1968 to h8_2, distance = 0.1\n",
      "Updating global tree: Added edge from h11_1 to h4_2, distance = 0.1\n",
      "Updating global tree: Added edge from h13_1 to h4_2, distance = 0.1\n",
      "Current MST has 93 nodes\n",
      "Current global tree has 103 vertices and 10 edges\n",
      "#################### Infer global tree: iteration 3 begins ####################\n",
      "##### Infer MP local tree: interation 1 begins #####\n",
      "Total number of unique site patterns is 125\n",
      "total number of informative sites is 98\n",
      "total number of unique informative_site_patterns is 72\n",
      "Total parsimony score is 346\n"
     ]
    }
   ],
   "source": [
    "#Record the starting time\n",
    "start_time = time.time()\n",
    "\n",
    "#Read the sequences\n",
    "sequences = {}\n",
    "jc_distances = {}\n",
    "\n",
    "fasta_file = open(fasta_file_path,'r')\n",
    "seq = ''\n",
    "seq_id = ''\n",
    "for line in fasta_file:\n",
    "  if line.startswith('>'):\n",
    "    seq_id = line.strip().split('>')[1]\n",
    "    seq = ''\n",
    "  else:\n",
    "    seq += line.strip()\n",
    "  if seq != '' and seq_id != '':\n",
    "    sequences[seq_id] = seq\n",
    "\n",
    "#Create the pairwise distance matrix\n",
    "dist_mat = create_distance_matrix(sequences)\n",
    "\n",
    "#Create the networkx-based MST\n",
    "MST = create_mst_from_dist_mat(sequences, dist_mat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "nx.draw(MST, with_labels=True, font_weight='bold', node_size = 500)\n",
    "fig.savefig('MST_starting.png')\n",
    "\n",
    "#Create an unrooted global phylogeny tree\n",
    "T_global = Tree(\"global\")\n",
    "T_global.Read_fasta_file(fasta_file_path)\n",
    "\n",
    "\n",
    "test = infer_global_tree(T_global, MST, sequences, W, threshold)\n",
    "df.to_csv(f'{len(sequences)}_{threshold}_stats.csv', encoding='utf-8', index=False)\n",
    "print(\"Elapsed time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5sJhpHtIPpF"
   },
   "source": [
    "###Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zn5b9EQQIWnv"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['k', 'MST nodes', \"Global tree vertices\", \"Global tree edges\", \"V size\"])\n",
    "k = 1\n",
    "while k < 7:\n",
    "  df.loc[k] = [1, 2, 3, 4, 5]\n",
    "  k += 1\n",
    "\n",
    "\n",
    "df.to_csv('test.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS54XkIfOAj9"
   },
   "source": [
    "##log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnhJPUSsOBze"
   },
   "outputs": [],
   "source": [
    "orig_stdout = sys.stdout\n",
    "sys.stdout = open('test.log', 'a')\n",
    "# sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixz08SR5OTgx"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "k = 1\n",
    "while k < 100:\n",
    "  k += 1\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9ipfD2FRXxqn",
    "H8_7Y5S7YDGl",
    "9kNufiyZYIqq",
    "37-2SuH-YNVV",
    "zXgjLaO6hJyL",
    "JEMk4nl5qnkV",
    "X0Ytj4L7W60z",
    "alNRA6pRW9br",
    "Ph9kqVS1XTso",
    "gJzkbdITXcMZ",
    "sMANpcGwXibl",
    "AVvKwT1Kj3hs",
    "8pB9hTU6kR3D",
    "tCcSNDygxyeD",
    "vucy9PpLbCee",
    "aL9BbJeNbHAD",
    "wpX7650hbNgK",
    "ulI5Rh1WcO9r",
    "IlsxKfv_8EMk",
    "0tbharifiRs8",
    "SCeD7ZGYjNx3",
    "09MBKyHV6nAB",
    "0BUthWfpmUzX",
    "UecCgloQ1ePx",
    "PHOX3t32ANoL",
    "dJrkJakUAjbf",
    "83TrJit6cKMP",
    "MSf1jZHIHchI",
    "-pvt4cTsHnk4",
    "Su35HDa5HuKb",
    "_8tdIKsQH2bR",
    "UzaCkIR1oB7x",
    "G5sJhpHtIPpF",
    "LS54XkIfOAj9",
    "T3LvMdHrcLSj",
    "GE-atSBa4FIb"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
